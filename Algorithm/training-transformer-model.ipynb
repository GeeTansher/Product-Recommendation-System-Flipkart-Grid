{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Models.py\nfrom typing import Optional\nimport pandas as pd\nimport pytorch_lightning as pl\nimport torch\nimport torch.nn as nn\nfrom torch.nn import Linear\nfrom torch.nn import functional as F\n\n\ndef masked_accuracy(y_pred: torch.Tensor, y_true: torch.Tensor, mask: torch.Tensor):\n\n    _, predicted = torch.max(y_pred, 1)\n\n    y_true = torch.masked_select(y_true, mask)\n    predicted = torch.masked_select(predicted, mask)\n\n    acc = (y_true == predicted).double().mean()\n\n    return acc\n\n\ndef masked_ce(y_pred, y_true, mask):\n\n    loss = F.cross_entropy(y_pred, y_true, reduction=\"none\")\n\n    loss = loss * mask\n\n    return loss.sum() / (mask.sum() + 1e-8)\n\n\nclass Recommender(pl.LightningModule):\n    def __init__(\n        self,\n        vocab_size,\n        channels=128,\n        cap=0,\n        mask=1,\n        dropout=0.4,\n        lr=1e-4,\n    ):\n        super().__init__()\n\n        self.cap = cap\n        self.mask = mask\n\n        self.lr = lr\n        self.dropout = dropout\n        self.vocab_size = vocab_size\n\n        self.item_embeddings = torch.nn.Embedding(\n            self.vocab_size, embedding_dim=channels\n        )\n\n        self.input_pos_embedding = torch.nn.Embedding(512, embedding_dim=channels)\n\n        encoder_layer = nn.TransformerEncoderLayer(\n            d_model=channels, nhead=4, dropout=self.dropout\n        )\n\n        self.encoder = torch.nn.TransformerEncoder(encoder_layer, num_layers=6)\n\n        self.linear_out = Linear(channels, self.vocab_size)\n\n        self.do = nn.Dropout(p=self.dropout)\n\n    def encode_src(self, src_items):\n        src_items = self.item_embeddings(src_items)\n\n        batch_size, in_sequence_len = src_items.size(0), src_items.size(1)\n        pos_encoder = (\n            torch.arange(0, in_sequence_len, device=src_items.device)\n            .unsqueeze(0)\n            .repeat(batch_size, 1)\n        )\n        pos_encoder = self.input_pos_embedding(pos_encoder)\n\n        src_items += pos_encoder\n\n        src = src_items.permute(1, 0, 2)\n\n        src = self.encoder(src)\n\n        return src.permute(1, 0, 2)\n\n    def forward(self, src_items):\n\n        src = self.encode_src(src_items)\n\n        out = self.linear_out(src)\n\n        return out\n\n    def training_step(self, batch, batch_idx):\n        src_items, y_true = batch\n\n        y_pred = self(src_items)\n\n        y_pred = y_pred.view(-1, y_pred.size(2))\n        y_true = y_true.view(-1)\n\n        src_items = src_items.view(-1)\n        mask = src_items == self.mask\n\n        loss = masked_ce(y_pred=y_pred, y_true=y_true, mask=mask)\n        accuracy = masked_accuracy(y_pred=y_pred, y_true=y_true, mask=mask)\n\n        self.log(\"train_loss\", loss)\n        self.log(\"train_accuracy\", accuracy)\n\n        return loss\n\n    def validation_step(self, batch, batch_idx):\n        src_items, y_true = batch\n\n        y_pred = self(src_items)\n\n        y_pred = y_pred.view(-1, y_pred.size(2))\n        y_true = y_true.view(-1)\n\n        src_items = src_items.view(-1)\n        mask = src_items == self.mask\n\n        loss = masked_ce(y_pred=y_pred, y_true=y_true, mask=mask)\n        accuracy = masked_accuracy(y_pred=y_pred, y_true=y_true, mask=mask)\n\n        self.log(\"valid_loss\", loss)\n        self.log(\"valid_accuracy\", accuracy)\n\n        return loss\n\n    def test_step(self, batch, batch_idx):\n        src_items, y_true = batch\n\n        y_pred = self(src_items)\n\n        y_pred = y_pred.view(-1, y_pred.size(2))\n        y_true = y_true.view(-1)\n\n        src_items = src_items.view(-1)\n        mask = src_items == self.mask\n\n        loss = masked_ce(y_pred=y_pred, y_true=y_true, mask=mask)\n        accuracy = masked_accuracy(y_pred=y_pred, y_true=y_true, mask=mask)\n\n        self.log(\"test_loss\", loss)\n        self.log(\"test_accuracy\", accuracy)\n\n        return loss\n\n    def configure_optimizers(self):\n        optimizer = torch.optim.Adam(self.parameters(), lr=self.lr)\n        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n            optimizer, patience=10, factor=0.1\n        )\n        return {\n            \"optimizer\": optimizer,\n            \"lr_scheduler\": scheduler,\n            \"monitor\": \"valid_loss\",\n        }\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-08-17T12:13:45.913914Z","iopub.execute_input":"2023-08-17T12:13:45.914266Z","iopub.status.idle":"2023-08-17T12:14:00.996248Z","shell.execute_reply.started":"2023-08-17T12:13:45.914237Z","shell.execute_reply":"2023-08-17T12:14:00.995282Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n  warnings.warn(f\"file system plugins are not loaded: {e}\")\n","output_type":"stream"}]},{"cell_type":"code","source":"import random\n\nimport numpy as np\nimport pandas as pd\n\nPAD = 0\nMASK = 1\n\n\ndef map_column(df: pd.DataFrame, col_name: str):\n    values = sorted(list(df[col_name].unique()))\n    mapping = {k: i + 2 for i, k in enumerate(values)}\n    inverse_mapping = {v: k for k, v in mapping.items()}\n\n    df[col_name + \"_mapped\"] = df[col_name].map(mapping)\n\n    return df, mapping, inverse_mapping\n\n\ndef get_context(df: pd.DataFrame, split: str, context_size: int = 120, val_context_size: int = 5):\n    if split == \"train\":\n        if val_context_size >= df.shape[0] - 10:\n            end_index = 10  # Set a reasonable fallback value\n        else:\n            end_index = random.randint(10, df.shape[0] - val_context_size)\n    elif split in [\"val\", \"test\"]:\n        end_index = df.shape[0]\n    else:\n        raise ValueError\n\n    start_index = max(0, end_index - context_size)\n\n    context = df[start_index:end_index]\n\n    return context\n\n\ndef pad_arr(arr: np.ndarray, expected_size: int = 30):\n    arr = np.pad(arr, [(expected_size - arr.shape[0], 0), (0, 0)], mode=\"edge\")\n    return arr\n\n\ndef pad_list(list_integers, history_size: int, pad_val: int = PAD, mode=\"left\"):\n    if len(list_integers) < history_size:\n        if mode == \"left\":\n            list_integers = [pad_val] * (history_size - len(list_integers)) + list_integers\n        else:\n            list_integers = list_integers + [pad_val] * (history_size - len(list_integers))\n\n    return list_integers\n\n\ndef df_to_np(df, expected_size=30):\n    arr = np.array(df)\n    arr = pad_arr(arr, expected_size=expected_size)\n    return arr\n\n","metadata":{"execution":{"iopub.status.busy":"2023-08-17T12:14:00.998181Z","iopub.execute_input":"2023-08-17T12:14:00.998514Z","iopub.status.idle":"2023-08-17T12:14:01.013603Z","shell.execute_reply.started":"2023-08-17T12:14:00.998481Z","shell.execute_reply":"2023-08-17T12:14:01.012523Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# training.py\nimport random\nimport pandas as pd\nimport pytorch_lightning as pl\nimport torch\nfrom pytorch_lightning.callbacks import ModelCheckpoint\nfrom pytorch_lightning.loggers import TensorBoardLogger\nfrom torch.utils.data import DataLoader\nfrom pathlib import Path\nimport numpy as np\n\ndef mask_last_elements_list(l1, val_context_size: int = 5):\n\n    l1 = l1[:-val_context_size] + mask_list(l1[-val_context_size:], p=0.5)\n\n    return l1\n\ndef mask_list(l1, p=0.8):\n\n    l1 = [a if random.random() < p else MASK for a in l1]\n\n    return l1\n\nclass Dataset(torch.utils.data.Dataset):\n    def __init__(self, groups, grp_by, split, history_size=120):\n        self.groups = groups\n        self.grp_by = grp_by\n        self.split = split\n        self.history_size = history_size\n\n    def __len__(self):\n        return len(self.groups)\n\n    def __getitem__(self, idx):\n        group = self.groups[idx]\n\n        df = self.grp_by.get_group(group)\n\n        context = get_context(df, split=self.split, context_size=self.history_size)\n\n        trg_items = context[\"product_id_mapped\"].tolist()\n\n        if self.split == \"train\":\n            src_items = mask_list(trg_items)\n        else:\n            src_items = mask_last_elements_list(trg_items)\n\n        pad_mode = \"left\" if random.random() < 0.5 else \"right\"\n        trg_items = pad_list(trg_items, history_size=self.history_size, mode=pad_mode)\n        src_items = pad_list(src_items, history_size=self.history_size, mode=pad_mode)\n\n        src_items = torch.tensor(src_items, dtype=torch.long)\n\n        trg_items = torch.tensor(trg_items, dtype=torch.long)\n\n        return src_items, trg_items","metadata":{"execution":{"iopub.status.busy":"2023-08-17T12:14:01.015020Z","iopub.execute_input":"2023-08-17T12:14:01.015507Z","iopub.status.idle":"2023-08-17T12:14:01.029377Z","shell.execute_reply.started":"2023-08-17T12:14:01.015475Z","shell.execute_reply":"2023-08-17T12:14:01.027331Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"def train(\n    data_csv_path: Path,\n    log_dir: str = \"/kaggle/working/recommender_logs\",\n    model_dir: str = \"/kaggle/working/recommender_models\",\n    batch_size: int = 32,\n    epochs: int = 2000,\n    history_size: int = 120,\n):\n    data = pd.read_csv(data_csv_path)\n    \n    data['review_date'] = pd.to_datetime(data['review_date'])\n\n    data['timestamp'] = data['review_date'].astype(np.int64) // 10**9\n    \n    # print(type(data['timestamp'][0]))\n\n    data.sort_values(by=\"timestamp\", inplace=True)\n\n    data, mapping, inverse_mapping = map_column(data, col_name=\"product_id\")\n\n    grp_by_train = data.groupby(by=\"customer_id\")\n\n    groups = list(grp_by_train.groups)\n\n    train_data = Dataset(\n        groups=groups,\n        grp_by=grp_by_train,\n        split=\"train\",\n        history_size=history_size,\n    )\n    val_data = Dataset(\n        groups=groups,\n        grp_by=grp_by_train,\n        split=\"val\",\n        history_size=history_size,\n    )\n\n    print(\"len(train_data)\", len(train_data))\n    print(\"len(val_data)\", len(val_data))\n\n    train_loader = DataLoader(\n        train_data,\n        batch_size=batch_size,\n        num_workers=10,\n        shuffle=True,\n    )\n    val_loader = DataLoader(\n        val_data,\n        batch_size=batch_size,\n        num_workers=10,\n        shuffle=False,\n    )\n\n    model = Recommender(\n        vocab_size=len(mapping) + 2,\n        lr=1e-4,\n        dropout=0.3,\n    )\n\n    logger = TensorBoardLogger(\n        save_dir=log_dir,\n    )\n\n    checkpoint_callback = ModelCheckpoint(\n        monitor=\"valid_loss\",\n        mode=\"min\",\n        dirpath=model_dir,\n        filename=\"recommender-jewellery\",\n    )\n\n    trainer = pl.Trainer(\n        max_epochs=epochs,\n        logger=logger,\n        callbacks=[checkpoint_callback],\n    )\n    trainer.fit(model, train_loader, val_loader)\n\n    result_val = trainer.test(dataloaders=val_loader)\n\n    output_json = {\n        \"val_loss\": result_val[0][\"test_loss\"],\n        \"best_model_path\": checkpoint_callback.best_model_path,\n    }\n\n    print(output_json)\n\n    return output_json","metadata":{"execution":{"iopub.status.busy":"2023-08-17T12:14:33.041309Z","iopub.execute_input":"2023-08-17T12:14:33.041945Z","iopub.status.idle":"2023-08-17T12:14:33.062217Z","shell.execute_reply.started":"2023-08-17T12:14:33.041901Z","shell.execute_reply":"2023-08-17T12:14:33.059949Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"train(\n        data_csv_path=Path(r\"/kaggle/input/augmented-dataset/Augmented_Jewelry.csv\"),\n        batch_size = 32,\n        epochs=150,\n    )","metadata":{"execution":{"iopub.status.busy":"2023-08-17T12:14:34.959480Z","iopub.execute_input":"2023-08-17T12:14:34.959953Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"len(train_data) 97\nlen(val_data) 97\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_28/1707247465.py:11: UserWarning: Parsing dates in DD/MM/YYYY format when dayfirst=False (the default) was specified. This may lead to inconsistently parsed dates! Specify a format to ensure consistent parsing.\n  data['review_date'] = pd.to_datetime(data['review_date'])\n/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n/opt/conda/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:615: UserWarning: Checkpoint directory /kaggle/working/recommender_models exists and is not empty.\n  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Sanity Checking: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py:280: PossibleUserWarning: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n  rank_zero_warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e96cead50d394cb78191b4106dadf0e9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}}]},{"cell_type":"code","source":"data = pd.read_csv(\"/kaggle/input/augmented-dataset/Augmented_Jewelry.csv\")\ndata.shape","metadata":{"execution":{"iopub.status.busy":"2023-08-17T12:14:05.187220Z","iopub.execute_input":"2023-08-17T12:14:05.187804Z","iopub.status.idle":"2023-08-17T12:14:05.208682Z","shell.execute_reply.started":"2023-08-17T12:14:05.187770Z","shell.execute_reply":"2023-08-17T12:14:05.207584Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"(1194, 6)"},"metadata":{}}]},{"cell_type":"code","source":"data.dropna()","metadata":{"execution":{"iopub.status.busy":"2023-08-17T09:22:16.338226Z","iopub.execute_input":"2023-08-17T09:22:16.338874Z","iopub.status.idle":"2023-08-17T09:22:16.360494Z","shell.execute_reply.started":"2023-08-17T09:22:16.338848Z","shell.execute_reply":"2023-08-17T09:22:16.359506Z"},"trusted":true},"execution_count":33,"outputs":[{"execution_count":33,"output_type":"execute_result","data":{"text/plain":"     customer_id       review_id  product_id  \\\n0       24371595  R27ZP1F1CD0C3Y  B004LLIL5A   \n1       42489718   RJ7RSBCHUDNNE  B004LLIKVU   \n2         861463  R1HVYBSKLQJI5S  B00IX1I3G6   \n3       25283295  R2HAXF0IIYQBIR  B00IX1I3G6   \n4         397970   RNYLPX611NB7Q  B005ESMGV4   \n..           ...             ...         ...   \n995      9033254  R1TS3MW501CTP6  B00B2TFURQ   \n996     29132432  R1Y3I0RJ1JZD53  BT00CTOY20   \n997     18985804  R1EDV9IZ0628IG  B00PG40CO4   \n998     30984333  R3ATXW3TX9TQM6  B00BWDH3VS   \n999     16003607  R2UZU8OYGE54XP  B00A48G0D4   \n\n                                         product_title product_category  \\\n0                        Amazon eGift Card - Celebrate        Gift Card   \n1                               Amazon.com eGift Cards        Gift Card   \n2                  Amazon.com Gift Card Balance Reload        Gift Card   \n3                  Amazon.com Gift Card Balance Reload        Gift Card   \n4    Amazon.com Gift Cards, Pack of 3 (Various Desi...        Gift Card   \n..                                                 ...              ...   \n995        Amazon Gift Card - Print - Thank You (Note)        Gift Card   \n996  Amazon.com Gift Card in a Greeting Card (Vario...        Gift Card   \n997     Amazon eGift Card - Happy Birthday (Doughnuts)        Gift Card   \n998                             Amazon.com eGift Cards        Gift Card   \n999       Amazon eGift Card - Happy Birthday (Candles)        Gift Card   \n\n     rating                                    review_headline  \\\n0         5                                         Five Stars   \n1         5  Gift card for the greatest selection of items ...   \n2         5                                         Five Stars   \n3         1                                           One Star   \n4         5                                         Five Stars   \n..      ...                                                ...   \n995       5                                         Five Stars   \n996       5                                 Works like a charm   \n997       5                                         Five Stars   \n998       5                                      Great Choice.   \n999       5                                         Five Stars   \n\n                                           review_body review_date  \n0               Great birthday gift for a young adult.  31-08-2015  \n1    It's an Amazon gift card and with over 9823983...  31-08-2015  \n2                                                 Good  31-08-2015  \n3                                                 Fair  31-08-2015  \n4    I can't believe how quickly Amazon can get the...  31-08-2015  \n..                                                 ...         ...  \n995                                       Awesome gift  31-08-2015  \n996                        Good as money on this site!  31-08-2015  \n997                                              great  31-08-2015  \n998  Suited the person I sent it to  well. Graphics...  31-08-2015  \n999                        EASY GIFT FOR STUDENTS AWAY  31-08-2015  \n\n[1000 rows x 9 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>customer_id</th>\n      <th>review_id</th>\n      <th>product_id</th>\n      <th>product_title</th>\n      <th>product_category</th>\n      <th>rating</th>\n      <th>review_headline</th>\n      <th>review_body</th>\n      <th>review_date</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>24371595</td>\n      <td>R27ZP1F1CD0C3Y</td>\n      <td>B004LLIL5A</td>\n      <td>Amazon eGift Card - Celebrate</td>\n      <td>Gift Card</td>\n      <td>5</td>\n      <td>Five Stars</td>\n      <td>Great birthday gift for a young adult.</td>\n      <td>31-08-2015</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>42489718</td>\n      <td>RJ7RSBCHUDNNE</td>\n      <td>B004LLIKVU</td>\n      <td>Amazon.com eGift Cards</td>\n      <td>Gift Card</td>\n      <td>5</td>\n      <td>Gift card for the greatest selection of items ...</td>\n      <td>It's an Amazon gift card and with over 9823983...</td>\n      <td>31-08-2015</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>861463</td>\n      <td>R1HVYBSKLQJI5S</td>\n      <td>B00IX1I3G6</td>\n      <td>Amazon.com Gift Card Balance Reload</td>\n      <td>Gift Card</td>\n      <td>5</td>\n      <td>Five Stars</td>\n      <td>Good</td>\n      <td>31-08-2015</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>25283295</td>\n      <td>R2HAXF0IIYQBIR</td>\n      <td>B00IX1I3G6</td>\n      <td>Amazon.com Gift Card Balance Reload</td>\n      <td>Gift Card</td>\n      <td>1</td>\n      <td>One Star</td>\n      <td>Fair</td>\n      <td>31-08-2015</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>397970</td>\n      <td>RNYLPX611NB7Q</td>\n      <td>B005ESMGV4</td>\n      <td>Amazon.com Gift Cards, Pack of 3 (Various Desi...</td>\n      <td>Gift Card</td>\n      <td>5</td>\n      <td>Five Stars</td>\n      <td>I can't believe how quickly Amazon can get the...</td>\n      <td>31-08-2015</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>995</th>\n      <td>9033254</td>\n      <td>R1TS3MW501CTP6</td>\n      <td>B00B2TFURQ</td>\n      <td>Amazon Gift Card - Print - Thank You (Note)</td>\n      <td>Gift Card</td>\n      <td>5</td>\n      <td>Five Stars</td>\n      <td>Awesome gift</td>\n      <td>31-08-2015</td>\n    </tr>\n    <tr>\n      <th>996</th>\n      <td>29132432</td>\n      <td>R1Y3I0RJ1JZD53</td>\n      <td>BT00CTOY20</td>\n      <td>Amazon.com Gift Card in a Greeting Card (Vario...</td>\n      <td>Gift Card</td>\n      <td>5</td>\n      <td>Works like a charm</td>\n      <td>Good as money on this site!</td>\n      <td>31-08-2015</td>\n    </tr>\n    <tr>\n      <th>997</th>\n      <td>18985804</td>\n      <td>R1EDV9IZ0628IG</td>\n      <td>B00PG40CO4</td>\n      <td>Amazon eGift Card - Happy Birthday (Doughnuts)</td>\n      <td>Gift Card</td>\n      <td>5</td>\n      <td>Five Stars</td>\n      <td>great</td>\n      <td>31-08-2015</td>\n    </tr>\n    <tr>\n      <th>998</th>\n      <td>30984333</td>\n      <td>R3ATXW3TX9TQM6</td>\n      <td>B00BWDH3VS</td>\n      <td>Amazon.com eGift Cards</td>\n      <td>Gift Card</td>\n      <td>5</td>\n      <td>Great Choice.</td>\n      <td>Suited the person I sent it to  well. Graphics...</td>\n      <td>31-08-2015</td>\n    </tr>\n    <tr>\n      <th>999</th>\n      <td>16003607</td>\n      <td>R2UZU8OYGE54XP</td>\n      <td>B00A48G0D4</td>\n      <td>Amazon eGift Card - Happy Birthday (Candles)</td>\n      <td>Gift Card</td>\n      <td>5</td>\n      <td>Five Stars</td>\n      <td>EASY GIFT FOR STUDENTS AWAY</td>\n      <td>31-08-2015</td>\n    </tr>\n  </tbody>\n</table>\n<p>1000 rows × 9 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"data.shape","metadata":{"execution":{"iopub.status.busy":"2023-08-17T09:22:16.362033Z","iopub.execute_input":"2023-08-17T09:22:16.362560Z","iopub.status.idle":"2023-08-17T09:22:16.369260Z","shell.execute_reply.started":"2023-08-17T09:22:16.362516Z","shell.execute_reply":"2023-08-17T09:22:16.368346Z"},"trusted":true},"execution_count":34,"outputs":[{"execution_count":34,"output_type":"execute_result","data":{"text/plain":"(1000, 9)"},"metadata":{}}]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport pytorch_lightning as pl\nimport torch\nfrom pytorch_lightning.callbacks import ModelCheckpoint\nfrom pytorch_lightning.loggers import TensorBoardLogger\nfrom torch.utils.data import DataLoader\n\ndata_csv_path = \"/kaggle/input/d/geetansher/grid-prs-amazon-dataset/Apparel.csv\"\nproducts_path = \"/kaggle/input/d/geetansher/grid-prs-amazon-dataset/Apparel.csv\"\n\nmodel_path = \"/kaggle/working/recommender_models/recommender-apparal.ckpt\"\n\n","metadata":{"execution":{"iopub.status.busy":"2023-08-17T09:22:16.370846Z","iopub.execute_input":"2023-08-17T09:22:16.371606Z","iopub.status.idle":"2023-08-17T09:22:16.378880Z","shell.execute_reply.started":"2023-08-17T09:22:16.371572Z","shell.execute_reply":"2023-08-17T09:22:16.377832Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"data = pd.read_csv(data_csv_path)\nproducts = pd.read_csv(products_path)","metadata":{"execution":{"iopub.status.busy":"2023-08-17T09:22:16.380806Z","iopub.execute_input":"2023-08-17T09:22:16.381925Z","iopub.status.idle":"2023-08-17T09:22:16.419356Z","shell.execute_reply.started":"2023-08-17T09:22:16.381891Z","shell.execute_reply":"2023-08-17T09:22:16.418495Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"data['review_date'] = pd.to_datetime(data['review_date'])\ndata['timestamp'] = data['review_date'].astype(np.int64) // 10**9\ndata.sort_values(by=\"timestamp\", inplace=True)\ndata, mapping, inverse_mapping = map_column(data, col_name=\"product_id\")\ngrp_by_train = data.groupby(by=\"customer_id\")","metadata":{"execution":{"iopub.status.busy":"2023-08-17T09:22:16.422461Z","iopub.execute_input":"2023-08-17T09:22:16.422722Z","iopub.status.idle":"2023-08-17T09:22:16.443617Z","shell.execute_reply.started":"2023-08-17T09:22:16.422698Z","shell.execute_reply":"2023-08-17T09:22:16.442750Z"},"trusted":true},"execution_count":37,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_28/3926916298.py:1: UserWarning: Parsing dates in DD/MM/YYYY format when dayfirst=False (the default) was specified. This may lead to inconsistently parsed dates! Specify a format to ensure consistent parsing.\n  data['review_date'] = pd.to_datetime(data['review_date'])\n","output_type":"stream"}]},{"cell_type":"code","source":"random.sample(list(grp_by_train.groups), k=10)","metadata":{"execution":{"iopub.status.busy":"2023-08-17T09:22:16.444826Z","iopub.execute_input":"2023-08-17T09:22:16.445711Z","iopub.status.idle":"2023-08-17T09:22:16.454579Z","shell.execute_reply.started":"2023-08-17T09:22:16.445678Z","shell.execute_reply":"2023-08-17T09:22:16.453538Z"},"trusted":true},"execution_count":38,"outputs":[{"execution_count":38,"output_type":"execute_result","data":{"text/plain":"[45912412,\n 46726859,\n 4105777,\n 2457386,\n 22745771,\n 48785098,\n 11003977,\n 935737,\n 511888,\n 45510794]"},"metadata":{}}]},{"cell_type":"code","source":"model = Recommender(\n        vocab_size=len(mapping) + 2,\n        lr=1e-4,\n        dropout=0.3,\n    )\nmodel.eval()\nmodel.load_state_dict(torch.load(model_path)[\"state_dict\"])","metadata":{"execution":{"iopub.status.busy":"2023-08-17T09:22:16.458573Z","iopub.execute_input":"2023-08-17T09:22:16.458837Z","iopub.status.idle":"2023-08-17T09:22:16.581981Z","shell.execute_reply.started":"2023-08-17T09:22:16.458814Z","shell.execute_reply":"2023-08-17T09:22:16.581014Z"},"trusted":true},"execution_count":39,"outputs":[{"execution_count":39,"output_type":"execute_result","data":{"text/plain":"<All keys matched successfully>"},"metadata":{}}]},{"cell_type":"code","source":"product_to_idx = {a: mapping[b] for a, b in zip(products.product_title.tolist(), products.product_id.tolist()) if b in mapping}\nidx_to_product = {v: k for k, v in product_to_idx.items()}","metadata":{"execution":{"iopub.status.busy":"2023-08-17T09:22:16.583306Z","iopub.execute_input":"2023-08-17T09:22:16.584036Z","iopub.status.idle":"2023-08-17T09:22:16.591280Z","shell.execute_reply.started":"2023-08-17T09:22:16.583981Z","shell.execute_reply":"2023-08-17T09:22:16.590383Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"def predict(list_products, model, product_to_idx, idx_to_product):\n    \n    ids = [PAD] * (120 - len(list_products) - 1) + [product_to_idx[a] for a in list_products] + [MASK]\n    \n    src = torch.tensor(ids, dtype=torch.long).unsqueeze(0)\n    \n    with torch.no_grad():\n        prediction = model(src)\n    \n    masked_pred = prediction[0, -1].numpy()\n    \n    sorted_predicted_ids = np.argsort(masked_pred).tolist()[::-1]\n    \n    sorted_predicted_ids = [a for a in sorted_predicted_ids if a not in ids]\n    \n    return [idx_to_product[a] for a in sorted_predicted_ids[:30] if a in idx_to_product]\n","metadata":{"execution":{"iopub.status.busy":"2023-08-17T09:22:16.594600Z","iopub.execute_input":"2023-08-17T09:22:16.594866Z","iopub.status.idle":"2023-08-17T09:22:16.603356Z","shell.execute_reply.started":"2023-08-17T09:22:16.594843Z","shell.execute_reply":"2023-08-17T09:22:16.602311Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"code","source":"# priniting product id\ndef predict_id(list_products_id, model, product_to_idx, idx_to_product):\n    \n    ids = [PAD] * (120 - len(list_products_id) - 1) + [a for a in list_products_id] + [MASK]\n    \n    src = torch.tensor(ids, dtype=torch.long).unsqueeze(0)\n    \n    with torch.no_grad():\n        prediction = model(src)\n    \n    masked_pred = prediction[0, -1].numpy()\n    \n    sorted_predicted_ids = np.argsort(masked_pred).tolist()[::-1]\n    \n    sorted_predicted_ids = [a for a in sorted_predicted_ids if a not in ids]\n    \n    return [a for a in sorted_predicted_ids[:30]]","metadata":{"execution":{"iopub.status.busy":"2023-08-17T09:22:16.604528Z","iopub.execute_input":"2023-08-17T09:22:16.605560Z","iopub.status.idle":"2023-08-17T09:22:16.614371Z","shell.execute_reply.started":"2023-08-17T09:22:16.605499Z","shell.execute_reply":"2023-08-17T09:22:16.613348Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"code","source":"list_products = [\"Levi's Boys' 514 Straight Fit Jeans\",\n               \"Jockey Women's Underwear Supersoft Brief - 3 Pack\",\n               \"Jerzees Men's Super Sweats Crew Neck Sweatshirt\",\n               \"SEOBEAN Mens Low Rise Sexy Sport Swimwear Trunk Boxer Brief Bikini Swimsuit\"]\n\ntop_products = predict(list_products, model, product_to_idx, idx_to_product)\ntop_products","metadata":{"execution":{"iopub.status.busy":"2023-08-17T09:22:16.615987Z","iopub.execute_input":"2023-08-17T09:22:16.616452Z","iopub.status.idle":"2023-08-17T09:22:16.649503Z","shell.execute_reply.started":"2023-08-17T09:22:16.616348Z","shell.execute_reply":"2023-08-17T09:22:16.648470Z"},"trusted":true},"execution_count":43,"outputs":[{"execution_count":43,"output_type":"execute_result","data":{"text/plain":"['ANGVNS Ladies Women Sexy Floor Length Strapless Long Dress for Party',\n 'Women Blue and Black Dresses under $25 for all Occasion',\n \"The North Face Women's ThermoBall Hybrid Hoodie - Magic Magenta\",\n \"Luouse Women Vintage 1950's Style 3/4 Sleeves Garden Floral Print Windbreaker Dress\",\n \"Norfolk Branded Men's Cushioned Running / Jogging Ankle Sports Socks - Owens\",\n 'SweetBridal Sweetheart Sleeveless Halter Evening Dress',\n \"Harriton Men's Barbados Textured Camp Shirt\",\n 'Robes King RK Classical Sleepwear Mens Broadcloth Woven Pajama Set',\n 'Womens Summer Open Shoulder Chiffon Shift Dress',\n \"JAEDEN Women's Beaded Spaghetti Straps Sexy Long Formal Prom Evening Dresses\",\n \"Glamorise Women's #1006 Full-Figure Sports Bra\",\n 'New Womens Long Sleeve Blouse Tops Round Neck Plaid Checked Loose Shirt',\n 'Amdirect 100W Manicure Pedicure Paraffin Warmer Waxing 400ml 220V Wax Heater Salon Spa',\n \"Match Men's Athletic Fit Straight Leg Casual Pants\",\n 'VIV Collection Best Selling Printed Brushed Leggings Plus Size (L - XXL) Listing 1',\n 'Jockey Scrubs Womens Maximum Comfort Pant',\n \"KRISP Women's Short Sleeve Pleated Front V-Neck Blouse Top Banded Hemline\",\n 'Amdirect Rotating Adjustable Black Metal Floor Stand 35 Hat 7 Tier Cap Display Retail Hat Rack',\n \"PUMA Men's BMW Team Polo\",\n \"Amdirect Musical Instrument Classic Acoustic Beginners Children's 6 String Toy Guitar\",\n 'Denim Shampoo - The Original Denim Wash',\n \"Lilyette Women's Enchantment Three-Section Unlined Minimizer Underwire Bra\",\n 'Deluxe Batman Gloves Costume Accessory',\n 'Easy Tool Stainless Steel Fruit Pineapple Corer Slicer Peeler Cut (One size, sliver)',\n 'Wookrays Heavy Duty Basketball Metal Chain Net Official Size Rims Hoop 10 Loop Steel Basketball Net',\n \"Gildan Men's Heavy Blend Crewneck Sweatshirt - Large - White\",\n \"James Fiallo Men's 12-Pairs Low Cut Athletic Sport Socks\",\n \"Deez Nuts for President 2016 Men's Humor Funny T-Shirt\",\n 'Amdirect Professional 252 Colors Ultimate Eyeshadow Palette Cosmetic Makeup Kit Box']"},"metadata":{}}]}]}